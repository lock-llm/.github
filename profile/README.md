# Welcome to the 1st üîíLock-LLM Workshop @ NeurIPS 25ÔºÅ

* **Website**: [lock-llm.github.io](https://lock-llm.github.io/)

* **Keynote Speakers**: [Yu Cheng](https://ych133.github.io/), [Charles Fleming](https://outshift.cisco.com/blog/author/charles-fleming), [Bhavya Kailkhura](https://people.llnl.gov/kailkhura1), [Zico Kolter](https://zicokolter.com/), [Huan Liu](https://faculty.engineering.asu.edu/huanliu), [Dawn Song](https://dawnsong.io/), [Atlas Wang](https://www.ece.utexas.edu/people/faculty/atlas-wang)

* **Organizers**: [Tianlong Chen](https://tianlong-chen.github.io), [Ang Li](https://www.ang-li.com/), [Furong Huang](https://www.cs.umd.edu/people/furongh), [Avi Schwarzschild](https://www.avischwarzschild.com/), [Neil Zhenqiang Gong](https://people.duke.edu/~zg70/), [Bo Li](https://aisecure.github.io/), [Yuxiong He](https://www.snowflake.com/en/blog/authors/yuxiong-he/)

* **Student Organizers**: [Pingzhi Li](https://pingzhili.github.io/), [Guoheng Sun](https://openreview.net/profile?id=~Guoheng_Sun1), [Zhen Tan](https://zhen-tan-dmml.github.io/), [Ziyao Wang](https://ziyaow-about.netlify.app), [Song Wang](https://songw-sw.github.io/)

### About
**The Challenge.** Large Language Models (LLMs) have emerged as transformative tools across research and industry, revolutionizing how we interact with information. However, their immense capabilities bring critical security challenges‚Äîthe same features that drive innovation can be exploited for malicious purposes through unauthorized distillation, fine-tuning, compression, or editing. These vulnerabilities pose severe threats including intellectual property theft, generation of sophisticated disinformation, bypass of safety alignments, and erosion of user trust in AI systems.

**Our Mission.** This workshop aims to bring together researchers and practitioners from academia and industry who are advancing the frontiers of LLM security and protection. We seek to confront the unauthorized use of LLMs head-on by exploring novel and robust mechanisms designed to make these models inherently resistant to exploitation while maintaining their beneficial capabilities.

Topics of interest include, but are not limited to:
1. Un-Distillate LLMs: Preventing unauthorized model replication and intellectual property theft
2. Un-Finetunable LLMs: Resisting malicious parameter updates and behavior alterations
3. Un-Compressible LLMs: Maintaining model integrity against unauthorized compression
4. Un-Editable LLMs: Safeguarding against knowledge tampering and misinformation injection
5. Un-Usable LLMs: Ensuring traceability and preventing misuse through watermarking and verification
